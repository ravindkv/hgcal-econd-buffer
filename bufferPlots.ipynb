{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of number of link that are assigned\n",
    "linkSummary = pd.read_csv('geomInfo/ModuleLinkSummary.csv')\n",
    "\n",
    "# six cassettes are put together to form one layer. Currently, we look at only 0th cassete of a few layers (5,7,9)\n",
    "linkSummary = linkSummary[(linkSummary.Cassette==0) & (linkSummary.Layer>=5) & (linkSummary.Layer<=9)]\n",
    "\n",
    "# The (ModU, modV) is the (U, V) coordinate of a module (=wafer=econ) within a layer\n",
    "linkSummary.set_index(['Layer','ModU','ModV'],inplace=True)\n",
    "linkSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from csv files.  This is used to extract average data sizes\n",
    "#daq_Data = pd.concat([pd.read_csv(f'Data/ttbar_copy_new.csv') for i in range(16)])\n",
    "daq_Data = pd.read_csv(f'Data/ttbar_copy_new.csv')\n",
    "'''\n",
    "The daq_Data consist of the entries after applying ZS corresponding to different cells \n",
    "(a layer has many wafers, a wafer has many cells). The description about the each of columns\n",
    "(c1): It is the index of df\n",
    "(c2 = entry): those events which passed the ZS\n",
    "(c3 = layer): layer number (from 5 to 9)\n",
    "(c4 = waferu): U coordinate of the wafer\n",
    "(c5 = waferv): V coordinate of the wafer\n",
    "\n",
    "(c6 = HDM): high density module: One of the 8-inch wafers/modules with 432 cells \n",
    "(compared to the LDM, or low-density module, which has 192 cells)\n",
    "the type 0 are 120 micron thick and HDM and closest to beam pipe,\n",
    "type 1 is 200 microns thick LDM modules, and a little further out\n",
    "type 2 are 300 micron LDM modules, and furthest from beam pipe in eta\n",
    "If HDM is True then it correspond to type 0 else 1 or 2\n",
    "\n",
    "(c7 = occ): Occupancy: Each wafer has 3-6 eLink, each eLink has up to 37 channels, \n",
    "weâ€™ve applied the zero suppression, so any channel below the threshold is removed, \n",
    "so the count is just counting how many cells on a given eLink on a given wafer pass the ZS\n",
    "\n",
    "(c8 = eRxPacket_Words): It is the size of the packet in terms of number of words (1 word is of 32 bit).\n",
    "It is calculated as int(Bits/32+1) + 2, where Bits = 16  + 8* (charge_BX1 + toa_BX2)\n",
    "\n",
    "(c9  = NonEmptyLinks): We have total 6 (12) eLinks for HDM(LDM). \n",
    "\n",
    "(c10 = EmptyLinks): The rest eLinks\n",
    "\n",
    "(c11 = TotalWords): evt_headerWords (=2) + eRxPacket_Words + EmptyLinks + evt_trailerWords (=2)\n",
    "\n",
    "''' \n",
    "daq_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daq_Data.groupby(['layer','waferu','waferv']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daq_Data.groupby(['layer','waferu','waferv']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of wedges, and assign x and y positions for drawing maps\n",
    "wedges = daq_Data.groupby(['layer','waferu','waferv']).any()[['HDM']].reset_index()\n",
    "wedges['y'] = wedges.waferv\n",
    "wedges['x'] = 0\n",
    "#QQQQ: why 2+v-2*u?\n",
    "wedges.loc[:,'x'] = (2+wedges.y-2*wedges.waferu)\n",
    "#QQQQ: why rescale y and x?\n",
    "wedges.y *= 1.5\n",
    "wedges.x *= -3**.5/2\n",
    "wedges.set_index(['layer','waferu','waferv'],inplace=True)\n",
    "wedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to look through output logs of condor jobs to extract and accumulate the results across several runs\n",
    "# if you just ran interactivel, need to just get the info on the overflows, max size, etc into a dataframe.\n",
    "\n",
    "def getDF(inputDF, LogName=None,Nstart=0,N=100):\n",
    "    #QQQQ: Why mean?\n",
    "    fullData = inputDF.groupby(['layer','waferu','waferv']).mean()\n",
    "    \n",
    "    #linkSummary has only 153 rows which means the last 10 rows will be assigned Nan. \n",
    "    #QQQQ: Why linkSummary has only 153 rows?\n",
    "    fullData['eTx_assigned'] = linkSummary.ECOND_eTX\n",
    "    \n",
    "    #Replace Nan by 1\n",
    "    fullData.eTx_assigned = fullData.eTx_assigned.fillna(1).astype(int)\n",
    "    \n",
    "    #The numpy.ceil() is a mathematical function that returns the ceil of the elements of array. \n",
    "    #The ceil of the scalar x is the smallest integer i, such that i >= x\n",
    "    #QQQQ: why divide by 53.3333? One eTx can process 53.3333 Words of data?\n",
    "    fullData['eTx_30percent'] = np.ceil((fullData.TotalWords/53.3333)*1.3).astype(int)\n",
    "    fullData['eTx_Mean'] = np.ceil((fullData.TotalWords/53.3333))\n",
    "    '''\n",
    "    Every condor output has information for 6 econs (they have different # eLinks)\n",
    "    We collect information from ALL condor outputs for ALL econs in a 2D (163,6) array\n",
    "\n",
    "    overflows: An econ with few number of eLink is overflow for a few thousands times in \n",
    "    few million BX which means for few billion BXs (many condor outputs) the overflows should be added\n",
    "\n",
    "    maxSize: Among all condor outouts, it is the maximum buffer size. Which means it is NOT additive\n",
    "\n",
    "    L1As issued: How many times an L1A is issued for a given number of BXs. Which means it is additive\n",
    "\n",
    "    '''\n",
    "    if not LogName is None:\n",
    "        data_overflow = []\n",
    "        data_maxSize = []\n",
    "        L1As_issued=0\n",
    "        \n",
    "        #Get the 0th element of the list from 0th condor file\n",
    "        fileName = f'log_100/{LogName}_{Nstart}.stdout'\n",
    "        with open(fileName,'r') as _file:\n",
    "            # we loop over all lines of the condor output and extract\n",
    "            for line in _file:\n",
    "                if 'overflows= ' in line:\n",
    "                    #overflows= [...]\n",
    "                    data_overflow.append(eval(line[10:]))\n",
    "                if 'maxSize= ' in line:\n",
    "                    #maxSize= [...]\n",
    "                    data_maxSize.append(eval(line[8:]))\n",
    "                if 'L1As issued' in line:\n",
    "                    #749401 L1As issued\n",
    "                    L1As_issued += int(line.split()[0])\n",
    "        #2D array            \n",
    "        data_overflow=np.array(data_overflow)\n",
    "        data_maxSize=np.array(data_maxSize)\n",
    "\n",
    "        for i_file in range(Nstart+1,Nstart+N):\n",
    "            fileName = f'log_100/{LogName}_{i_file}.stdout'\n",
    "            with open(fileName,'r') as _file:\n",
    "                for line in _file:\n",
    "                    if 'eTx' in line:\n",
    "                        #1 eTx\n",
    "                        i = int(line.split()[0])-1\n",
    "                    if 'overflows= ' in line:\n",
    "                        overflow=np.array(eval(line[10:]))\n",
    "                        overflow[overflow<0] = 99999999\n",
    "                        data_overflow[i] += overflow\n",
    "                    if 'maxSize= ' in line:\n",
    "                        maxSize=np.array(eval(line[8:]))\n",
    "                        data_maxSize[i] = np.maximum(data_maxSize[i], maxSize)\n",
    "                    if 'L1As issued' in line:\n",
    "                        L1As_issued += int(line.split()[0])\n",
    "        \n",
    "        '''\n",
    "        The idea is to check how many eLinks are needed for a given wafer in order to avoid overflow.\n",
    "        For example, the wafer closer to beam axis need more eLinks and vice versa. In this way we can\n",
    "        efficiantly allocate different number of eLinks to different wafers\n",
    "        '''\n",
    "        #transpose (163 col, 6 rows) to (6 cols, 163 rows)\n",
    "        #In every row look at zero (True==0) and non-zero entries\n",
    "        #Multiply the respective entry by array([1, 2, 3, 4, 5, 6])\n",
    "        x = ((data_overflow.transpose()==0)*np.arange(1,7))\n",
    "\n",
    "        #change all zero entries to 99 (we can choose any large value)\n",
    "        x[x==0]=99\n",
    "        \n",
    "        #for every row, get the minimum value (out of 6 econs, pick the one which has minimum eLink)\n",
    "        minLinks = x.min(axis=1)\n",
    "        fullData['min_eTx'] = minLinks\n",
    "        nLinks = minLinks\n",
    "        \n",
    "        #Get the maximum size of the buffer of the corresponding econ with minLinks\n",
    "        maxSize = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][nLinks[i]-1])\n",
    "        fullData['maxSize'] = maxSize    \n",
    "\n",
    "        \n",
    "        nLinks = fullData.eTx_assigned.values\n",
    "        maxSize = []\n",
    "        overflows = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][nLinks[i]-1])\n",
    "            overflows.append(data_overflow.transpose()[i][nLinks[i]-1])\n",
    "        fullData['maxSize_assigned'] = maxSize\n",
    "        fullData['overflows_assigned'] = overflows\n",
    "\n",
    "\n",
    "        nLinks = fullData.eTx_30percent.values\n",
    "        maxSize = []\n",
    "        overflows = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][nLinks[i]-1])\n",
    "            overflows.append(data_overflow.transpose()[i][nLinks[i]-1])\n",
    "        fullData['maxSize_30percent'] = maxSize\n",
    "        fullData['overflows_30percent'] = overflows\n",
    "        print(\"fullData = \", fullData)\n",
    "        #print(\"L1As_issued = \", L1As_issued)\n",
    "        #print(\"data_overflow = \", data_overflow)\n",
    "        #print(\"data_maxSize = \", data_maxSize)\n",
    "    return fullData, L1As_issued, data_overflow, data_maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData,L1As,OF, MS = getDF(daq_Data,LogName='bufferSim__34530053',Nstart=0,N=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = f'log_buffHist.stdout'\n",
    "with open(fileName,'r') as _file:\n",
    "    for line in _file:\n",
    "        if 'buffHist= ' in line:\n",
    "            #overflows= [...]\n",
    "            buffHist = eval(line[9:])\n",
    "#print(buffHist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "count = 1\n",
    "plt.subplots(figsize=(15, 10))\n",
    "#for h in range(len(buffHist)):\n",
    "for h in range(20):\n",
    "    #print(count)\n",
    "    plt.subplot(4, 5, count)\n",
    "    plt.hist(buffHist[h], bins=20, range=[0.0, 1550], label=\"wafer=%i\"%h)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.yscale('log')\n",
    "    count+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(OF[0], bins=70, alpha=0.5, label=\"1 eTx\", fill=False, linewidth=3, edgecolor='c',range=[0.0, 0.7e7])\n",
    "plt.hist(OF[1], bins=70, alpha=0.5, label=\"2 eTx\", fill=False, linewidth=3, edgecolor='b',range=[0.0, 0.7e7])\n",
    "plt.hist(OF[2], bins=70, alpha=0.5, label=\"3 eTx\", fill=False, linewidth=3, edgecolor='g',range=[0.0, 0.7e7])\n",
    "plt.hist(OF[3], bins=70, alpha=0.5, label=\"4 eTx\", fill=False, linewidth=3, edgecolor='r',range=[0.0, 0.7e7])\n",
    "plt.hist(OF[4], bins=70, alpha=0.5, label=\"5 eTx\", fill=False, linewidth=3, edgecolor='m',range=[0.0, 0.7e7])\n",
    "plt.hist(OF[5], bins=70, alpha=0.5, label=\"6 eTx\", fill=False, linewidth=3, edgecolor='y',range=[0.0, 0.7e7])\n",
    "plt.xlabel(\"Overflows\", size=14)\n",
    "plt.ylabel(\"Number of Wafers\", size=14)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Overflows for different numbers of eTx\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(MS[0], bins=20, alpha=0.5, label=\"1 eTx\", fill=False, linewidth=3, edgecolor='c',range=[0.0, 2000])\n",
    "plt.hist(MS[1], bins=20, alpha=0.5, label=\"2 eTx\", fill=False, linewidth=3, edgecolor='b',range=[0.0, 2000])\n",
    "plt.hist(MS[2], bins=20, alpha=0.5, label=\"3 eTx\", fill=False, linewidth=3, edgecolor='g',range=[0.0, 2000])\n",
    "plt.hist(MS[3], bins=20, alpha=0.5, label=\"4 eTx\", fill=False, linewidth=3, edgecolor='r',range=[0.0, 2000])\n",
    "plt.hist(MS[4], bins=20, alpha=0.5, label=\"5 eTx\", fill=False, linewidth=3, edgecolor='m',range=[0.0, 2000])\n",
    "plt.hist(MS[5], bins=20, alpha=0.5, label=\"6 eTx\", fill=False, linewidth=3, edgecolor='y',range=[0.0, 2000])\n",
    "plt.xlabel(\"Max buffer size\", size=14)\n",
    "plt.ylabel(\"Number of Wafers\", size=14)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Max buffer size for different numbers of eTx\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaly of the labels for different types of plots\n",
    "Labels ={'occ':{'Title':'Average L1A Occupancy, Layer %i',\n",
    "                'colorLabel':'Avg. Occ. Above Zero Suppression',\n",
    "                'nDec':1,\n",
    "                'zMax':260},\n",
    "         'TotalWords':{'Title':'Average L1A Size (32b Words), Layer %i',\n",
    "                      'colorLabel':'Avg. L1A Size (32b words)',\n",
    "                      'nDec':1,\n",
    "                      'zMax':200},\n",
    "         'EmptyLinks':{'Title':'Average Number of Empty eRx Packets, Layer %i',\n",
    "                      'colorLabel':'Avg. # of Empty eRx Packets',\n",
    "                      'nDec':1,\n",
    "                      'zMax':6},\n",
    "         'eTx_assigned':{'Title':'Number of eTx assigned, Layer %i',\n",
    "                      'colorLabel':'# of eTx',\n",
    "                      'nDec':0,\n",
    "                      'zMax':6},\n",
    "         'eTx_30percent':{'Title':'Number of eTx assuming 30 percent overhead, Layer %i',\n",
    "                      'colorLabel':'# of eTx',\n",
    "                      'nDec':0,\n",
    "                      'zMax':6},\n",
    "         'eTx_Mean':{'Title':'Number of eTx based on mean data, no rounding, Layer %i',\n",
    "                      'colorLabel':'# of eTx',\n",
    "                      'nDec':2,\n",
    "                      'zMax':6},\n",
    "         'min_eTx':{'Title':'Min number of eTx with 0 overflows, Layer %i',\n",
    "                      'colorLabel':'# of eTx',\n",
    "                      'nDec':0,\n",
    "                      'zMax':6},\n",
    "         'maxSize':{'Title':'Max buffer size, with 0 overflows, Layer %i',\n",
    "                      'colorLabel':'Buffer size (32b words)',\n",
    "                      'nDec':0,\n",
    "                      'zMax':1600},\n",
    "         'maxSize_assigned':{'Title':'Max buffer size, using assigned number of links, Layer %i',\n",
    "                      'colorLabel':'Buffer size (32b words)',\n",
    "                      'nDec':0,\n",
    "                      'zMax':1600},\n",
    "         'overflows_assigned':{'Title':'Overflow counter, using assigned number of links, Layer %i',\n",
    "                      'colorLabel':'# of buffer overflows',\n",
    "                      'nDec':0,\n",
    "                      'zMax':40000},\n",
    "         'maxSize_30percent':{'Title':'Max buffer size, using link assignment from 30 percent overhead, Layer %i',\n",
    "                      'colorLabel':'Buffer size (32b words)',\n",
    "                      'nDec':0,\n",
    "                      'zMax':1600},\n",
    "         'overflows_30percent':{'Title':'Overflow counter, using link assignment from 30 percent overhead, Layer %i',\n",
    "                      'colorLabel':'# of buffer overflows',\n",
    "                      'nDec':0,\n",
    "                      'zMax':40000},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlot(var,patches,df=fullData,extra=\"\"):\n",
    "    data = df.loc[layer,var].values.flatten()\n",
    "\n",
    "    waferCollection = PatchCollection(patches,cmap=matplotlib.cm.coolwarm)\n",
    "    waferCollection.set_array(data)\n",
    "    waferCollection.set_clim([0,Labels[var]['zMax']])\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.add_collection(waferCollection)\n",
    "    plt.axis([0,16,-1,15])\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    # ax.set_xticklabels([])\n",
    "    # ax.set_yticklabels([]);\n",
    "    cbar = plt.colorbar(waferCollection)\n",
    "    cbar.set_label(Labels[var]['colorLabel'],fontsize=14)\n",
    "\n",
    "    wedge['data'] = data\n",
    "\n",
    "    plt.title(Labels[var]['Title']%layer,fontsize=24)\n",
    "    for x,y,d in wedge[['x','y','data']].values:\n",
    "    #     s = data[i]\n",
    "        plt.text(x,y,f'%.{Labels[var][\"nDec\"]}f'%d,fontsize=14,horizontalalignment='center',verticalalignment='center')\n",
    "\n",
    "                 \n",
    "\n",
    "    fig.savefig(f'Plots/{var}_layer{layer}{extra}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in [5,7,9]:\n",
    "for layer in [5]:\n",
    "    wedge = wedges.loc[layer]\n",
    "    patches = []\n",
    "    for w in wedge.index:\n",
    "        patches.append(mpatches.RegularPolygon((wedge.loc[w].x,wedge.loc[w].y),6, .95))\n",
    "    for k in Labels:\n",
    "        print(k)\n",
    "        makePlot(k,df=fullData,extra=\"_OldTTbarTests\",patches=patches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
