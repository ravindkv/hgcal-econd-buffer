{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "sys.path.insert(0, \"%s/hgcalEnv/lib/python3.6/site-packages/\"%os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of number of link that are assigned\n",
    "linkSummary = pd.read_csv('geomInfo/ModuleLinkSummary.csv')\n",
    "\n",
    "# six cassettes are put together to form one layer. Currently, we look at only 0th cassete of a few layers (5,7,9)\n",
    "linkSummary = linkSummary[(linkSummary.Cassette==0) & (linkSummary.Layer>=5) & (linkSummary.Layer<=9)]\n",
    "\n",
    "# The (ModU, modV) is the (U, V) coordinate of a module (=wafer=econ) within a layer\n",
    "linkSummary.set_index(['Layer','ModU','ModV'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load data from csv files.  This is used to extract average data sizes\n",
    "#daq_Data = pd.concat([pd.read_csv(f'Data/ttbar_copy_new.csv') for i in range(16)])\n",
    "daq_Data = pd.read_csv('dataZS_merged.csv')\n",
    "'''\n",
    "The daq_Data consist of the entries after applying ZS corresponding to different cells \n",
    "(a layer has many wafers, a wafer has many cells). The description about the each of columns\n",
    "(c1): It is the index of df\n",
    "(c2 = entry): those events which passed the ZS\n",
    "(c3 = layer): layer number (from 5 to 9)\n",
    "(c4 = waferu): U coordinate of the wafer\n",
    "(c5 = waferv): V coordinate of the wafer\n",
    "\n",
    "(c6 = HDM): high density module: One of the 8-inch wafers/modules with 432 cells \n",
    "(compared to the LDM, or low-density module, which has 192 cells)\n",
    "the type 0 are 120 micron thick and HDM and closest to beam pipe,\n",
    "type 1 is 200 microns thick LDM modules, and a little further out\n",
    "type 2 are 300 micron LDM modules, and furthest from beam pipe in eta\n",
    "If HDM is True then it correspond to type 0 else 1 or 2\n",
    "\n",
    "(c7 = occ): Occupancy: Each wafer has 3-6 eLink, each eLink has up to 37 channels, \n",
    "weâ€™ve applied the zero suppression, so any channel below the threshold is removed, \n",
    "so the count is just counting how many cells on a given eLink on a given wafer pass the ZS\n",
    "\n",
    "(c8 = eRxPacket_Words): It is the size of the packet in terms of number of words (1 word is of 32 bit).\n",
    "It is calculated as int(Bits/32+1) + 2, where Bits = 16  + 8* (charge_BX1 + toa_BX2)\n",
    "\n",
    "(c9  = NonEmptyLinks): We have total 6 (12) eLinks for HDM(LDM). \n",
    "\n",
    "(c10 = EmptyLinks): The rest eLinks\n",
    "\n",
    "(c11 = TotalWords): evt_headerWords (=2) + eRxPacket_Words + EmptyLinks + evt_trailerWords (=2)\n",
    "\n",
    "''' \n",
    "#daq_Data\n",
    "#get list of wedges, and assign x and y positions for drawing maps\n",
    "wedges = daq_Data.groupby(['layer','waferu','waferv']).any()[['HDM']].reset_index()\n",
    "wedges['y'] = wedges.waferv\n",
    "wedges['x'] = 0\n",
    "#QQQQ: why 2+v-2*u?\n",
    "wedges.loc[:,'x'] = (2+wedges.y-2*wedges.waferu)\n",
    "#QQQQ: why rescale y and x?\n",
    "wedges.y *= 1.5\n",
    "wedges.x *= -3**.5/2\n",
    "wedges.set_index(['layer','waferu','waferv'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to look through output logs of condor jobs to extract and accumulate the results across several runs\n",
    "# if you just ran interactivel, need to just get the info on the overflows, max size, etc into a dataframe.\n",
    "\n",
    "def getDF(inputDF, LogDir=\"log\", LogName=None,Nstart=0,N=100):\n",
    "    #QQQQ: Why mean?\n",
    "    fullData = inputDF.groupby(['layer','waferu','waferv']).mean()\n",
    "    \n",
    "    #linkSummary has only 153 rows which means the last 10 rows will be assigned Nan. \n",
    "    #QQQQ: Why linkSummary has only 153 rows?\n",
    "    fullData['eTx_assigned'] = linkSummary.ECOND_eTX\n",
    "    \n",
    "    #Replace Nan by 1\n",
    "    fullData.eTx_assigned = fullData.eTx_assigned.fillna(1).astype(int)\n",
    "    \n",
    "    #The numpy.ceil() is a mathematical function that returns the ceil of the elements of array. \n",
    "    #The ceil of the scalar x is the smallest integer i, such that i >= x\n",
    "    #QQQQ: why divide by 53.3333? One eTx can process 53.3333 Words of data?\n",
    "    fullData['eTx_Mean'] = np.ceil((fullData.TotalWords/53.3333))\n",
    "    fullData['eTx_30percent'] = np.ceil((fullData.TotalWords/53.3333)*1.3).astype(int)\n",
    "    fullData['eTx_30percent_actual'] = fullData['eTx_30percent']/(fullData.TotalWords/53.3333)\n",
    "    '''\n",
    "    Every condor output has information for 6 econs (they have different # eLinks)\n",
    "    We collect information from ALL condor outputs for ALL econs in a 2D (163,6) array\n",
    "\n",
    "    overflows: An econ with few number of eLink is overflown for a few thousands times in \n",
    "    few million BX which means for few billion BXs (many condor outputs) the overflows should be added\n",
    "\n",
    "    maxSize: Among all condor outputs, it is the maximum buffer size. Which means it is NOT additive\n",
    "\n",
    "    L1As issued: How many times an L1A is issued for a given number of BXs. Which means it is additive\n",
    "\n",
    "    '''\n",
    "    if not LogName is None:\n",
    "        data_overflow = []\n",
    "        data_maxSize = []\n",
    "        L1As_issued=0\n",
    "        bHist = []\n",
    "        #Get the 0th element of the list from 0th condor file\n",
    "        fileName = f'condor_buffer/tmpSub/{LogDir}/{LogName}_{Nstart}.stdout'\n",
    "        with open(fileName,'r') as _file:\n",
    "            # we loop over all lines of the condor output and extract\n",
    "            for line in _file:\n",
    "                if 'overflows= ' in line:\n",
    "                    #overflows= [...]\n",
    "                    data_overflow.append(eval(line[10:]))\n",
    "                if 'maxSize= ' in line:\n",
    "                    #maxSize= [...]\n",
    "                    data_maxSize.append(eval(line[8:]))\n",
    "                if 'L1As issued' in line:\n",
    "                    #749401 L1As issued\n",
    "                    L1As_issued += int(line.split()[0])\n",
    "                if 'sizeHist= ' in line:\n",
    "                    bHist.append(eval(line[9:]))\n",
    "        #2D array            \n",
    "        data_overflow=np.array(data_overflow)\n",
    "        data_maxSize=np.array(data_maxSize)\n",
    "        buff_Hist = np.array(bHist)\n",
    "\n",
    "        for i_file in range(Nstart+1,Nstart+N):\n",
    "            fileName = f'condor_buffer/tmpSub/{LogDir}/{LogName}_{i_file}.stdout'\n",
    "            bHist = []\n",
    "            with open(fileName,'r') as _file:\n",
    "                for line in _file:\n",
    "                    if 'eTx' in line:\n",
    "                        #1 eTx\n",
    "                        i = int(line.split()[0])-1\n",
    "                    if 'overflows= ' in line:\n",
    "                        overflow=np.array(eval(line[10:]))\n",
    "                        overflow[overflow<0] = 99999999\n",
    "                        data_overflow[i] += overflow\n",
    "                    if 'maxSize= ' in line:\n",
    "                        maxSize=np.array(eval(line[8:]))\n",
    "                        data_maxSize[i] = np.maximum(data_maxSize[i], maxSize)\n",
    "                    if 'L1As issued' in line:\n",
    "                        L1As_issued += int(line.split()[0])\n",
    "                    if 'sizeHist= ' in line:\n",
    "                        bHist.append(eval(line[9:]))\n",
    "            buff_Hist += np.array(bHist)\n",
    "        \n",
    "        '''\n",
    "        The idea is to check how many eLinks are needed for a given wafer in order to avoid overflow.\n",
    "        For example, the wafer closer to beam axis need more eLinks and vice versa. In this way we can\n",
    "        efficiantly allocate different number of eLinks to different wafers\n",
    "        '''\n",
    "        #transpose (163 col, 6 rows) to (6 cols, 163 rows)\n",
    "        #In every row look at zero (True==0) and non-zero entries\n",
    "        #Multiply the respective entry by array([1, 2, 3, 4, 5, 6])\n",
    "        x = ((data_overflow.transpose()==0)*np.arange(1,7))\n",
    "\n",
    "        #change all zero entries to 99 (we can choose any large value)\n",
    "        x[x==0]=99\n",
    "        \n",
    "        #for every row, get the minimum value (out of 6 econs, pick the one which has minimum eLink)\n",
    "        minLinks = x.min(axis=1)\n",
    "        fullData['min_eTx'] = minLinks\n",
    "        nLinks = minLinks\n",
    "        #print(data_overflow.transpose())\n",
    "        \n",
    "        #Get the maximum size of the buffer of the corresponding econ with minLinks\n",
    "        maxSize = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][nLinks[i]-1])\n",
    "        fullData['maxSize'] = maxSize    \n",
    "\n",
    "        \n",
    "        nLinks = fullData.eTx_Mean.values\n",
    "        maxSize = []\n",
    "        overflows = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][int(nLinks[i])-1])\n",
    "            overflows.append(data_overflow.transpose()[i][int(nLinks[i])-1])\n",
    "        fullData['maxSize_Mean'] = maxSize\n",
    "        fullData['overflows_Mean'] = overflows\n",
    "        \n",
    "        \n",
    "        nLinks = fullData.eTx_assigned.values\n",
    "        maxSize = []\n",
    "        overflows = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][nLinks[i]-1])\n",
    "            overflows.append(data_overflow.transpose()[i][nLinks[i]-1])\n",
    "        fullData['maxSize_assigned'] = maxSize\n",
    "        fullData['overflows_assigned'] = overflows\n",
    "        \n",
    "\n",
    "        nLinks = fullData.eTx_30percent.values\n",
    "        maxSize = []\n",
    "        overflows = []\n",
    "        for i in range(len(nLinks)):\n",
    "            maxSize.append(data_maxSize.transpose()[i][nLinks[i]-1])\n",
    "            overflows.append(data_overflow.transpose()[i][nLinks[i]-1])\n",
    "        fullData['maxSize_30percent'] = maxSize\n",
    "        fullData['overflows_30percent'] = overflows\n",
    "        #print(\"fullData = \", fullData)\n",
    "        #print(\"L1As_issued = \", L1As_issued)\n",
    "        #print(\"data_overflow = \", data_overflow)\n",
    "        #print(\"data_maxSize = \", data_maxSize)\n",
    "    #return fullData, L1As_issued, data_overflow, data_maxSize, buff_Hist\n",
    "    #print(\"buff_Hist\", buff_Hist)\n",
    "    return fullData, L1As_issued, buff_Hist, data_overflow, data_maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBuffHist(buffHist):\n",
    "    colList = [\"c\", \"b\", \"g\", \"r\",\"m\",\"y\"]\n",
    "    xArray = []\n",
    "    for i in range(1536*2):\n",
    "        xArray.append(i)\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    count = 1\n",
    "    plt.subplots(figsize=(15, 10))\n",
    "    #for h in range(len(buffHist)):\n",
    "    for h in range(5):\n",
    "        #print(count)\n",
    "        plt.subplot(3, 5, count)\n",
    "        for eTx in range(len(buffHist)):\n",
    "            #plt.plot( xArray, buffHist[eTx][h], label=\"eTx=%i\"%(eTx+1))\n",
    "            plt.hist(xArray, bins=50, weights=buffHist[eTx][h], edgecolor=colList[eTx], label=\"eTx=%i\"%(eTx+1), histtype='step', linewidth=2)\n",
    "            plt.title(\"wafer=%s\"%str(wedges.index[h]))\n",
    "            #plt.xlabel(\"Buffer size (32-bit words)\", size=14)\n",
    "            #plt.xlabel(\"Buffer size\", size=14)\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.yscale('log')\n",
    "        count+=1\n",
    "    plt.show()\n",
    "\n",
    "def plotSize(sizeList, linkList, buffHist, plotTit, m):\n",
    "    xArray = []\n",
    "    for i in range(1536*2):\n",
    "        xArray.append(i)\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    plt.subplots(figsize=(25, 10))\n",
    "    #indices of maximum buffer size\n",
    "    #Return the index of original list after sorting it\n",
    "    index5 = sorted(range(len(m[0:53])), key=lambda k: m[0:53][k])\n",
    "    index7 = sorted(range(len(m[53:108])), key=lambda k: m[53:108][k])\n",
    "    index9 = sorted(range(len(m[108:163])), key=lambda k: m[108:163][k])\n",
    "    \n",
    "    layers = {}\n",
    "    layers[\"layer5\"] = [index5[-1], index5[-2], index5[0]]\n",
    "    layers[\"layer7\"] = [53+index7[-1], 53+index7[-2], 53+index7[0]]\n",
    "    layers[\"layer9\"] = [108+index9[-1], 108+index9[-2], 108+index9[0]]\n",
    "    count = 1\n",
    "    yMin  = 1\n",
    "    for layer in layers.keys():\n",
    "        plt.subplot(3, 5, count)\n",
    "        for wafer in layers[layer]:\n",
    "            hist = buffHist[int(linkList[wafer])-1][wafer]\n",
    "            lab = \"â„“, wu, wv = %s; eTx = %s, max= %s\"%(wedges.index[wafer], int(linkList[wafer]), sizeList[wafer])\n",
    "            plt.hist(xArray, bins=50, weights=hist, label=lab, histtype='step', linewidth=3)\n",
    "            plt.xlabel(\"Buffer size (32-bit words)\", size=14)\n",
    "            plt.ylabel(\"BX\", size=14)\n",
    "            plt.ylim(yMin, 10e12)\n",
    "            plt.legend(loc='upper center')\n",
    "            plt.yscale('log')\n",
    "        count+=1\n",
    "        plt.title(plotTit)\n",
    "        plt.axvline(x=1536, color=\"r\", linestyle='--')\n",
    "        plt.axvline(x=3072, color=\"r\", linestyle='--')\n",
    "        plt.text(1536+50, 10e4*yMin, \"128-word depth\", rotation=90, verticalalignment='center')\n",
    "        plt.text(3072+50, 10e4*yMin, \"256-word depth\", rotation=90, verticalalignment='center')\n",
    "    plt.show()\n",
    "    \n",
    "def plotMaxSize(xList, yList, xTit, yTit, plotTit, drawLine=False): \n",
    "    ETx = np.array(xList)\n",
    "    l1_ETx = ETx[0:53]\n",
    "    l2_ETx = ETx[53:53+55]\n",
    "    l3_ETx = ETx[53+55:53+55+55]\n",
    "    l1_overflow = []\n",
    "    l2_overflow = []\n",
    "    l3_overflow = []\n",
    "    for i in np.arange(len(l1_ETx)):\n",
    "        eTx = int(l1_ETx[i])\n",
    "        l1_overflow.append(yList.transpose()[i][eTx-1])\n",
    "\n",
    "    for i in np.arange(len(l2_ETx)):\n",
    "        eTx = int(l2_ETx[i])\n",
    "        l2_overflow.append(yList.transpose()[i+53][eTx-1])\n",
    "\n",
    "    for i in np.arange(len(l3_ETx)):\n",
    "        eTx = int(l3_ETx[i])\n",
    "        l3_overflow.append(yList.transpose()[i+53+55][eTx-1])\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(l1_overflow, bins=70,  label=\"â„“ = 5 (53 wafers)     \", histtype='step', linewidth=3, edgecolor='r')\n",
    "    plt.hist(l2_overflow, bins=70,  label=\"â„“ = 7 (55 wafers)     \", histtype='step', linewidth=3, edgecolor='g')\n",
    "    plt.hist(l3_overflow, bins=70,  label=\"â„“ = 9 (55 wafers)     \", histtype='step', linewidth=3, edgecolor='b')\n",
    "    plt.xlabel(xTit, size=14)\n",
    "    plt.ylabel(yTit, size=14)\n",
    "    plt.ylim(0.1, 60)\n",
    "    plt.title(plotTit)\n",
    "    plt.legend(loc=\"upper right\", frameon=False)\n",
    "    plt.yscale('log')\n",
    "    if drawLine:\n",
    "        plt.axvline(x=1536, color=\"r\", linestyle='--')\n",
    "        plt.axvline(x=3072, color=\"r\", linestyle='--')\n",
    "        plt.text(1536+50, 5, \"128-word depth\", rotation=90, verticalalignment='center')\n",
    "        plt.text(3072+50, 5, \"256-word depth\", rotation=90, verticalalignment='center')\n",
    "        \n",
    "\n",
    "def plotTotalWords(sizeList):   \n",
    "    branchList = ['entry','layer','waferu','waferv','HDM','TotalWords', \"TotalWords_NZS\"]\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    plt.subplots(figsize=(15, 10))\n",
    "    group = daq_Data.reset_index().groupby(['layer', 'waferu','waferv', 'HDM'])\n",
    "    histDF = group['TotalWords'].apply(list).reset_index()\n",
    "    histDF2 = group['TotalWords_NZS'].apply(list).reset_index()\n",
    "    count = 1\n",
    "    #Return the index of original list after sorting it\n",
    "    index5 = sorted(range(len(sizeList[0:53])), key=lambda k: sizeList[0:53][k])\n",
    "    index7 = sorted(range(len(sizeList[53:108])), key=lambda k: sizeList[53:108][k])\n",
    "    index9 = sorted(range(len(sizeList[108:163])), key=lambda k: sizeList[108:163][k])\n",
    "    \n",
    "    layers = {}\n",
    "    layers[\"layer5\"] = [index5[-1], index5[-2], index5[0]]\n",
    "    layers[\"layer7\"] = [53+index7[-1], 53+index7[-2], 53+index7[0]]\n",
    "    layers[\"layer9\"] = [108+index9[-1], 108+index9[-2], 108+index9[0]]\n",
    "    for layer in layers.keys():\n",
    "        for wafer in layers[layer]:\n",
    "            #print(count)\n",
    "            #luv = random.choice(wedges.index)\n",
    "            luv = wedges.index[wafer]\n",
    "            print(luv)\n",
    "            plt.subplot(3, 5, count)\n",
    "            sel = (histDF.layer==luv[0]) & (histDF.waferu==luv[1]) & (histDF.waferv==luv[2])\n",
    "            histList = histDF.loc[sel, 'TotalWords'].to_list()\n",
    "            histList_NZS = histDF2.loc[sel, 'TotalWords_NZS'].to_list()\n",
    "            #print(histList)\n",
    "            #print(histList_NZS)\n",
    "            plt.hist(histList, label=\"ZS\", histtype='step', linewidth=2)\n",
    "            plt.hist(histList_NZS, label=\"NZS\", histtype='step', linewidth=2)\n",
    "            plt.title(\"(layer, wu, wv)=%s\"%str(luv))\n",
    "            plt.xlabel(\"Total 32-bit words\", size=14)\n",
    "            plt.ylabel(\"Events\", size=14)\n",
    "            plt.legend(loc='upper center')\n",
    "            #plt.yscale('log')\n",
    "            count+=1\n",
    "    plt.show()\n",
    "    \n",
    "def plotOverFlow(OF):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(OF[0], bins=70,  label=\"1 eTx\", fill=False, linewidth=3, edgecolor='c')\n",
    "    plt.hist(OF[1], bins=70,  label=\"2 eTx\", fill=False, linewidth=3, edgecolor='b')\n",
    "    plt.hist(OF[2], bins=70,  label=\"3 eTx\", fill=False, linewidth=3, edgecolor='g')\n",
    "    plt.hist(OF[3], bins=70,  label=\"4 eTx\", fill=False, linewidth=3, edgecolor='r')\n",
    "    plt.hist(OF[4], bins=70,  label=\"5 eTx\", fill=False, linewidth=3, edgecolor='m')\n",
    "    plt.hist(OF[5], bins=70,  label=\"6 eTx\", fill=False, linewidth=3, edgecolor='y')\n",
    "    plt.xlabel(\"Overflows\", size=14)\n",
    "    plt.ylabel(\"Number of Wafers\", size=14)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Overflows for different numbers of eTx\")\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "def plot2D(fullData):\n",
    "    #dictionaly of the labels for different types of plots\n",
    "    Labels ={'occ':{'Title':'Layer %i',\n",
    "                    'colorLabel':'Average cell occupancy above ZS',\n",
    "                    'nDec':1,\n",
    "                    'zMax':260},\n",
    "             'TotalWords':{'Title':'Layer %i',\n",
    "                          'colorLabel':'Average data size (32b words) after L1A',\n",
    "                          'nDec':1,\n",
    "                          'zMax':200},\n",
    "             'EmptyLinks':{'Title':'Layer %i',\n",
    "                          'colorLabel':'Average # of empty eRx packets',\n",
    "                          'nDec':1,\n",
    "                          'zMax':6},\n",
    "             'min_eTx':{'Title':'Layer %i',\n",
    "                          'colorLabel':'Minimum number of eTx with 0 overflow',\n",
    "                          'nDec':0,\n",
    "                          'zMax':6},\n",
    "             'maxSize':{'Title':'Layer %i',\n",
    "                          'colorLabel':'Max buffer size (MBS) with 0 overflow',\n",
    "                          'nDec':0,\n",
    "                          'zMax':100},\n",
    "             'eTx_Mean':{'Title':'Layer %i',\n",
    "                          'colorLabel':'#eTx from mean data',\n",
    "                          'nDec':2,\n",
    "                          'zMax':6},\n",
    "             'eTx_30percent':{'Title':'Layer %i',\n",
    "                          'colorLabel':'#eTx+30% from mean data',\n",
    "                          'nDec':0,\n",
    "                          'zMax':6},\n",
    "             'eTx_30percent_actual':{'Title':'Layer %i',\n",
    "                          'colorLabel':'actual overhead for #eTx+30% assignment',\n",
    "                          'nDec':0,\n",
    "                          'zMax':6},             \n",
    "             'eTx_assigned':{'Title':'Layer %i',\n",
    "                          'colorLabel':'#eTx assigned from HGCal baseline',\n",
    "                          'nDec':0,\n",
    "                          'zMax':6},\n",
    "             'maxSize_Mean':{'Title':'Layer %i',\n",
    "                          'colorLabel':'MBS for #eTx from mean data',\n",
    "                          'nDec':0,\n",
    "                          'zMax':100},\n",
    "             'maxSize_30percent':{'Title':'Layer %i',\n",
    "                          'colorLabel':'MBS for #eTx+30% from mean data',\n",
    "                          'nDec':0,\n",
    "                          'zMax':100},\n",
    "             'maxSize_assigned':{'Title':'Layer %i',\n",
    "                          'colorLabel':'MBS for #eTx from HGCal baseline',\n",
    "                          'nDec':0,\n",
    "                          'zMax':100},\n",
    "             'overflows_Mean':{'Title':'Layer %i',\n",
    "                          'colorLabel':'OC for #eTx from mean data',\n",
    "                          'nDec':0,\n",
    "                          'zMax':40000},\n",
    "             'overflows_30percent':{'Title':'Layer %i',\n",
    "                          'colorLabel':'OC for #eTx+30% from mean data',\n",
    "                          'nDec':0,\n",
    "                          'zMax':40000},\n",
    "             'overflows_assigned':{'Title':'Layer %i',\n",
    "                          'colorLabel':'OC for #eTx from HGCal baseline',\n",
    "                          'nDec':0,\n",
    "                          'zMax':40000},\n",
    "            }\n",
    "    \n",
    "    #for var in Labels:\n",
    "    #for var in [\"eTx_Mean\", \"eTx_30percent\",  \"eTx_assigned\", \"eTx_30percent_actual\"]:\n",
    "    for var in [\"eTx_assigned\", \"maxSize_assigned\", \"overflows_assigned\", \"eTx_Mean\", \"maxSize_Mean\", \"overflows_Mean\", \"eTx_30percent\", \"maxSize_30percent\", \"overflows_30percent\", \"eTx_30percent_actual\"]:    \n",
    "        print(var)\n",
    "        plt.subplots(figsize=(15, 5))\n",
    "        count=1\n",
    "        for layer in [5,7,9]:\n",
    "            wedge = wedges.loc[layer]\n",
    "            #fig, ax = plt.subplots()\n",
    "            plt.subplot(1, 3, count)\n",
    "            ax=plt.gca()\n",
    "            patches = []\n",
    "            for w in wedge.index:\n",
    "                patches.append(mpatches.RegularPolygon((wedge.loc[w].x,wedge.loc[w].y),6, .95))\n",
    "            extra = \"ttbar\"\n",
    "            data = fullData.loc[layer,var].values.flatten()\n",
    "            if 'maxSize' in var:\n",
    "                data = data*100/3072\n",
    "            waferCollection = PatchCollection(patches,cmap=matplotlib.cm.coolwarm)\n",
    "            waferCollection.set_array(data)\n",
    "            waferCollection.set_clim([0,Labels[var]['zMax']])\n",
    "            ax.add_collection(waferCollection)\n",
    "            plt.axis([0,16,-1,15])\n",
    "            plt.rcParams[\"figure.autolayout\"] = True\n",
    "            # ax.set_xticklabels([])\n",
    "            # ax.set_yticklabels([]);\n",
    "            cbar = plt.colorbar(waferCollection)\n",
    "            cbar.set_label(Labels[var]['colorLabel'],fontsize=14)\n",
    "\n",
    "            wedge['data'] = data\n",
    "            plt.title(Labels[var]['Title']%layer,fontsize=14)\n",
    "            for x,y,d in wedge[['x','y','data']].values:\n",
    "            #     s = data[i]\n",
    "                text = f'%.{Labels[var][\"nDec\"]}f'%d\n",
    "                if len(str(d))>4:\n",
    "                    text = \"{:.2e}\".format(d)\n",
    "                    text = \"%s\\ne%s\"%(text.split(\"e\")[0], text.split(\"e\")[1])\n",
    "                if \"maxSize\" in var:\n",
    "                    per = round(d,1)\n",
    "                    text = \"%s\\n%s\"%(per, \"%\")\n",
    "                if \"actual\" in var:\n",
    "                    per = round(d,2)\n",
    "                    text = \"%s\"%(per)  \n",
    "                plt.text(x,y,text,fontsize=9,horizontalalignment='center',verticalalignment='center')         \n",
    "\n",
    "            #fig.savefig(f'Plots/{var}_layer{layer}{extra}.png')\n",
    "            count+=1\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData, L1As, buffHist, OF, MS = getDF(daq_Data, 'log_0', 'bufferSim__74754635', 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData2, L1As2, buffHist2, OF2, MS2 = getDF(daq_Data, 'log_100', 'bufferSim__74754636', 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullData3, L1As3, buffHist3, OF3, MS3 = getDF(daq_Data, 'log_1000', 'bufferSim__74754637', 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotAll(fullData, L1As, buffHist, OF, MS):\n",
    "    eTxMean = \"#eTx from mean data\"\n",
    "    eTx30p  = \"#eTx+30% from mean data\"\n",
    "    eTxHGCal= \"#eTx from HGCal baseline\"\n",
    "    yTit = \"Number of Wafers\"\n",
    "    xTit = \"Number of times buffer has Overflown\"\n",
    "    \n",
    "    #plotMaxSize(fullData.eTx_Mean.tolist(), OF, xTit, yTit, eTxMean)\n",
    "    #plotMaxSize(fullData.eTx_30percent.tolist(), OF, xTit, yTit, eTx30p)\n",
    "    #plotMaxSize(fullData.eTx_assigned.tolist(), OF, xTit, yTit, eTxHGCal)\n",
    "\n",
    "    yTit = \"Number of Wafers\"\n",
    "    xTit = \"Max buffer size (32-bit words)\"\n",
    "    plotMaxSize(fullData.eTx_Mean.tolist(), MS, xTit, yTit, eTxMean, True)\n",
    "    plotMaxSize(fullData.eTx_30percent.tolist(), MS, xTit, yTit, eTx30p, True)\n",
    "    plotMaxSize(fullData.eTx_assigned.tolist(), MS, xTit, yTit, eTxHGCal, True)\n",
    "    #To pick modules based on maxSize from 30%+mean data\n",
    "    m = fullData.maxSize_30percent.values\n",
    "    plotSize(fullData.maxSize_Mean.values, fullData.eTx_Mean.values, buffHist, eTxMean, m)\n",
    "    plotSize(fullData.maxSize_30percent.values, fullData.eTx_30percent.values, buffHist, eTx30p, m)\n",
    "    plotSize(fullData.maxSize_assigned.values, fullData.eTx_assigned.values, buffHist, eTxHGCal, m)\n",
    "\n",
    "    #plotBuffHist(buffHist)\n",
    "    plot2D(fullData)\n",
    "    plotTotalWords(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotAll(fullData, L1As, buffHist, OF, MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAll(fullData2, L1As2, buffHist2, OF2, MS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
